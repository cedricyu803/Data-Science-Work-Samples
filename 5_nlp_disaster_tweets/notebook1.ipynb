{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-10-23T23:54:48.197727Z",
     "iopub.status.busy": "2021-10-23T23:54:48.197011Z",
     "iopub.status.idle": "2021-10-23T23:54:48.215117Z",
     "shell.execute_reply": "2021-10-23T23:54:48.214314Z",
     "shell.execute_reply.started": "2021-10-23T23:54:48.197637Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Make the output look better\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.options.mode.chained_assignment = None  # default='warn' # ignores warning about dropping columns inplace\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.chdir(r'C:\\Users\\Cedric Yu\\Desktop\\Works\\9_nlp_disaster_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "#             line = line.strip().split()\n",
    "            line = re.split(r' ', line.rstrip())\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        # starts from 1\n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index is valued  from 1 to 1193514\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.twitter.27B/glove.twitter.27B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, index_to_word[i]) for i, vec in enumerate(list(word_to_vec_map.values())) if len(vec) != 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, index_to_word[i]) for i, vec in enumerate(list(word_to_vec_map.values())) if type(vec) != np.ndarray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\x85',\n",
       " array([-0.32053  , -0.73053  , -0.15227  ,  0.75504  ,  0.11011  ,\n",
       "         0.14091  ,  0.047278 , -1.0087   ,  0.13282  , -0.10939  ,\n",
       "        -0.010336 ,  0.32189  , -1.4589   , -0.83385  , -0.52429  ,\n",
       "         0.55353  ,  0.054966 ,  0.02489  ,  0.066947 ,  0.39403  ,\n",
       "        -0.2942   , -1.2322   , -0.2594   , -0.72149  ,  0.3671   ,\n",
       "         0.24201  ,  0.023268 ,  0.14087  ,  0.60309  ,  0.37282  ,\n",
       "         0.40474  ,  0.16387  ,  1.5523   , -0.28782  , -0.26105  ,\n",
       "        -0.83564  , -0.031021 ,  0.26182  , -0.093516 , -0.36343  ,\n",
       "        -0.10013  , -0.113    ,  1.3461   , -0.7571   , -0.51527  ,\n",
       "        -0.0099121, -0.34748  ,  0.2534   ,  0.43839  ,  0.30234  ,\n",
       "         0.0080009,  0.51505  ,  0.25082  , -0.53778  ,  0.20495  ,\n",
       "         0.27272  ,  0.13311  ,  0.98437  , -0.24143  ,  0.041526 ,\n",
       "         0.21953  , -0.20118  ,  0.068255 ,  1.2481   , -0.28648  ,\n",
       "        -0.058264 ,  0.18604  ,  0.45244  ,  0.36555  ,  0.35107  ,\n",
       "         0.78051  , -0.20271  ,  0.99956  , -0.4688   , -0.49431  ,\n",
       "        -0.14843  ,  0.0022548, -0.10625  , -0.21541  , -0.24243  ,\n",
       "        -0.68123  , -0.17896  , -0.86271  ,  0.74024  ,  0.73827  ,\n",
       "         0.4905   , -0.71627  , -0.49518  ,  0.050481 , -0.21521  ,\n",
       "        -1.21     ,  0.38652  , -0.22538  ,  0.52208  ,  0.62189  ,\n",
       "         0.44918  , -0.229    ,  0.045921 ,  0.73164  , -0.23074  ]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_vec_map.items())[38522]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.63006  ,  0.65177  ,  0.25545  ,  0.018593 ,  0.043094 ,\n",
       "        0.047194 ,  0.23218  ,  0.11613  ,  0.17371  ,  0.40487  ,\n",
       "        0.022524 , -0.076731 , -2.2911   ,  0.094127 ,  0.43293  ,\n",
       "        0.041801 ,  0.063175 , -0.64486  , -0.43657  ,  0.024114 ,\n",
       "       -0.082989 ,  0.21686  , -0.13462  , -0.22336  ,  0.39436  ,\n",
       "       -2.1724   , -0.39544  ,  0.16536  ,  0.39438  , -0.35182  ,\n",
       "       -0.14996  ,  0.10502  , -0.45937  ,  0.27729  ,  0.8924   ,\n",
       "       -0.042313 , -0.009345 ,  0.55017  ,  0.095521 ,  0.070504 ,\n",
       "       -1.1781   ,  0.013723 ,  0.17742  ,  0.74142  ,  0.17716  ,\n",
       "        0.038468 , -0.31684  ,  0.08941  ,  0.20557  , -0.34328  ,\n",
       "       -0.64303  , -0.878    , -0.16293  , -0.055925 ,  0.33898  ,\n",
       "        0.60664  , -0.2774   ,  0.33626  ,  0.21603  , -0.11051  ,\n",
       "        0.0058673, -0.64757  , -0.068222 , -0.77414  ,  0.13911  ,\n",
       "       -0.15851  , -0.61885  , -0.10192  , -0.47     ,  0.19787  ,\n",
       "        0.42175  , -0.18458  ,  0.080581 , -0.22545  , -0.065129 ,\n",
       "       -0.15328  ,  0.087726 , -0.18817  , -0.08371  ,  0.21779  ,\n",
       "        0.97899  ,  0.1092   ,  0.022705 , -0.078234 ,  0.15595  ,\n",
       "        0.083105 , -0.6824   ,  0.57469  , -0.19942  ,  0.50566  ,\n",
       "       -0.18277  ,  0.37721  , -0.12514  , -0.42821  , -0.81075  ,\n",
       "       -0.39326  , -0.17386  ,  0.55096  ,  0.64706  , -0.6093   ])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vec_map['<user>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sujet 1.2106 -0.70863 -0.97707 0.33535 0.8257 0.36896 -0.021482 -0.27034 -0.0055492 0.83869 -0.0897 -0.79128 -0.56701 0.04908 -0.11738 0.16432 -0.30619 0.33262 -0.63908 0.63574 0.98825 -0.59295 0.75938 0.024022 -0.60007 -0.14378 0.03236 -0.009005 0.20658 0.39102 0.42084 -0.24475 -1.1626 -0.013978 0.34457 -0.10953 -0.77528 -0.022399 1.8653 -0.41061 0.28008 -0.18468 0.64054 0.82215 -0.79153 0.029881 0.095458 0.026657 0.39674 -0.33538 1.6431 0.88992 -0.16722 -0.33381 -0.30418 -0.18491 -0.31889 -0.36405 -0.28864 -0.67385 0.52207 -0.81557 0.50586 -0.13545 0.50805 0.27894 0.87376 0.7039 -0.35322 -0.49251 -0.52577 0.64415 -0.019044 0.074907 -0.85638 -0.36872 0.21592 1.1508 0.042314 -0.26352 -1.0528 0.37033 0.12375 -0.71449 -0.12852 0.51006 0.017738 -0.21601 0.44469 0.43843 -0.79782 0.17197 -0.52321 0.040046 0.017109 0.80263 0.36342 0.45538 -0.90831 0.26275\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2 = open('glove.twitter.27B/glove.twitter.27B.100d.txt', 'r', encoding=\"utf8\")\n",
    "for i in range(38521):\n",
    "    f2.readline()\n",
    "f2.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x85 -0.32053 -0.73053 -0.15227 0.75504 0.11011 0.14091 0.047278 -1.0087 0.13282 -0.10939 -0.010336 0.32189 -1.4589 -0.83385 -0.52429 0.55353 0.054966 0.02489 0.066947 0.39403 -0.2942 -1.2322 -0.2594 -0.72149 0.3671 0.24201 0.023268 0.14087 0.60309 0.37282 0.40474 0.16387 1.5523 -0.28782 -0.26105 -0.83564 -0.031021 0.26182 -0.093516 -0.36343 -0.10013 -0.113 1.3461 -0.7571 -0.51527 -0.0099121 -0.34748 0.2534 0.43839 0.30234 0.0080009 0.51505 0.25082 -0.53778 0.20495 0.27272 0.13311 0.98437 -0.24143 0.041526 0.21953 -0.20118 0.068255 1.2481 -0.28648 -0.058264 0.18604 0.45244 0.36555 0.35107 0.78051 -0.20271 0.99956 -0.4688 -0.49431 -0.14843 0.0022548 -0.10625 -0.21541 -0.24243 -0.68123 -0.17896 -0.86271 0.74024 0.73827 0.4905 -0.71627 -0.49518 0.050481 -0.21521 -1.21 0.38652 -0.22538 0.52208 0.62189 0.44918 -0.229 0.045921 0.73164 -0.23074\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'تجيك -0.0086072 -0.045552 0.76923 0.33665 0.47002 0.20591 0.58329 0.39912 -0.84918 0.4524 -0.69694 -0.47102 1.5725 -0.38347 -0.69467 0.23937 -0.068866 -0.49309 0.22208 -0.32475 0.067222 -0.16051 -0.35697 1.1171 -0.21472 0.8607 -1.0533 0.13233 -0.079707 -1.4735 -0.27194 -0.51637 -0.093469 0.77393 0.73855 -0.5891 0.67086 -0.15636 0.81749 -0.16279 0.086272 -0.31285 0.43453 -0.37779 0.35951 -0.32811 -0.71145 0.56341 0.10842 0.89888 -1.3576 0.045787 0.53593 -0.19954 -0.5318 0.6122 0.06773 0.55234 0.76693 -0.56384 0.54085 0.24356 -0.34695 -0.078237 0.27433 -0.5992 0.060716 0.26403 0.64524 -0.38895 -0.77658 -0.61518 -0.2976 -0.95616 1.0183 0.96244 -0.71142 -0.57008 0.58891 0.15711 0.42677 -0.75565 0.43331 -0.071171 0.36887 -1.1318 0.91042 -0.25877 -0.39821 -0.59412 -0.54999 0.2178 -0.30694 1.016 0.15137 -0.033104 -0.067191 1.0566 0.87215 -0.63009\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(38522, '-0.32053')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, list(word_to_vec_map.keys())[i]) for i, vec in enumerate(list(word_to_vec_map.values())) if len(vec) != 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.73053  , -0.15227  ,  0.75504  ,  0.11011  ,  0.14091  ,\n",
       "        0.047278 , -1.0087   ,  0.13282  , -0.10939  , -0.010336 ,\n",
       "        0.32189  , -1.4589   , -0.83385  , -0.52429  ,  0.55353  ,\n",
       "        0.054966 ,  0.02489  ,  0.066947 ,  0.39403  , -0.2942   ,\n",
       "       -1.2322   , -0.2594   , -0.72149  ,  0.3671   ,  0.24201  ,\n",
       "        0.023268 ,  0.14087  ,  0.60309  ,  0.37282  ,  0.40474  ,\n",
       "        0.16387  ,  1.5523   , -0.28782  , -0.26105  , -0.83564  ,\n",
       "       -0.031021 ,  0.26182  , -0.093516 , -0.36343  , -0.10013  ,\n",
       "       -0.113    ,  1.3461   , -0.7571   , -0.51527  , -0.0099121,\n",
       "       -0.34748  ,  0.2534   ,  0.43839  ,  0.30234  ,  0.0080009,\n",
       "        0.51505  ,  0.25082  , -0.53778  ,  0.20495  ,  0.27272  ,\n",
       "        0.13311  ,  0.98437  , -0.24143  ,  0.041526 ,  0.21953  ,\n",
       "       -0.20118  ,  0.068255 ,  1.2481   , -0.28648  , -0.058264 ,\n",
       "        0.18604  ,  0.45244  ,  0.36555  ,  0.35107  ,  0.78051  ,\n",
       "       -0.20271  ,  0.99956  , -0.4688   , -0.49431  , -0.14843  ,\n",
       "        0.0022548, -0.10625  , -0.21541  , -0.24243  , -0.68123  ,\n",
       "       -0.17896  , -0.86271  ,  0.74024  ,  0.73827  ,  0.4905   ,\n",
       "       -0.71627  , -0.49518  ,  0.050481 , -0.21521  , -1.21     ,\n",
       "        0.38652  , -0.22538  ,  0.52208  ,  0.62189  ,  0.44918  ,\n",
       "       -0.229    ,  0.045921 ,  0.73164  , -0.23074  ])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vec_map['-0.32053']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "line00 = '\\x85 -0.32053 -0.73053 -0.15227 0.75504 0.11011 0.14091 0.047278 -1.0087 0.13282 -0.10939 -0.010336 0.32189 -1.4589 -0.83385 -0.52429 0.55353 0.054966 0.02489 0.066947 0.39403 -0.2942 -1.2322 -0.2594 -0.72149 0.3671 0.24201 0.023268 0.14087 0.60309 0.37282 0.40474 0.16387 1.5523 -0.28782 -0.26105 -0.83564 -0.031021 0.26182 -0.093516 -0.36343 -0.10013 -0.113 1.3461 -0.7571 -0.51527 -0.0099121 -0.34748 0.2534 0.43839 0.30234 0.0080009 0.51505 0.25082 -0.53778 0.20495 0.27272 0.13311 0.98437 -0.24143 0.041526 0.21953 -0.20118 0.068255 1.2481 -0.28648 -0.058264 0.18604 0.45244 0.36555 0.35107 0.78051 -0.20271 0.99956 -0.4688 -0.49431 -0.14843 0.0022548 -0.10625 -0.21541 -0.24243 -0.68123 -0.17896 -0.86271 0.74024 0.73827 0.4905 -0.71627 -0.49518 0.050481 -0.21521 -1.21 0.38652 -0.22538 0.52208 0.62189 0.44918 -0.229 0.045921 0.73164 -0.23074\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\x85',\n",
       " '-0.32053',\n",
       " '-0.73053',\n",
       " '-0.15227',\n",
       " '0.75504',\n",
       " '0.11011',\n",
       " '0.14091',\n",
       " '0.047278',\n",
       " '-1.0087',\n",
       " '0.13282',\n",
       " '-0.10939',\n",
       " '-0.010336',\n",
       " '0.32189',\n",
       " '-1.4589',\n",
       " '-0.83385',\n",
       " '-0.52429',\n",
       " '0.55353',\n",
       " '0.054966',\n",
       " '0.02489',\n",
       " '0.066947',\n",
       " '0.39403',\n",
       " '-0.2942',\n",
       " '-1.2322',\n",
       " '-0.2594',\n",
       " '-0.72149',\n",
       " '0.3671',\n",
       " '0.24201',\n",
       " '0.023268',\n",
       " '0.14087',\n",
       " '0.60309',\n",
       " '0.37282',\n",
       " '0.40474',\n",
       " '0.16387',\n",
       " '1.5523',\n",
       " '-0.28782',\n",
       " '-0.26105',\n",
       " '-0.83564',\n",
       " '-0.031021',\n",
       " '0.26182',\n",
       " '-0.093516',\n",
       " '-0.36343',\n",
       " '-0.10013',\n",
       " '-0.113',\n",
       " '1.3461',\n",
       " '-0.7571',\n",
       " '-0.51527',\n",
       " '-0.0099121',\n",
       " '-0.34748',\n",
       " '0.2534',\n",
       " '0.43839',\n",
       " '0.30234',\n",
       " '0.0080009',\n",
       " '0.51505',\n",
       " '0.25082',\n",
       " '-0.53778',\n",
       " '0.20495',\n",
       " '0.27272',\n",
       " '0.13311',\n",
       " '0.98437',\n",
       " '-0.24143',\n",
       " '0.041526',\n",
       " '0.21953',\n",
       " '-0.20118',\n",
       " '0.068255',\n",
       " '1.2481',\n",
       " '-0.28648',\n",
       " '-0.058264',\n",
       " '0.18604',\n",
       " '0.45244',\n",
       " '0.36555',\n",
       " '0.35107',\n",
       " '0.78051',\n",
       " '-0.20271',\n",
       " '0.99956',\n",
       " '-0.4688',\n",
       " '-0.49431',\n",
       " '-0.14843',\n",
       " '0.0022548',\n",
       " '-0.10625',\n",
       " '-0.21541',\n",
       " '-0.24243',\n",
       " '-0.68123',\n",
       " '-0.17896',\n",
       " '-0.86271',\n",
       " '0.74024',\n",
       " '0.73827',\n",
       " '0.4905',\n",
       " '-0.71627',\n",
       " '-0.49518',\n",
       " '0.050481',\n",
       " '-0.21521',\n",
       " '-1.21',\n",
       " '0.38652',\n",
       " '-0.22538',\n",
       " '0.52208',\n",
       " '0.62189',\n",
       " '0.44918',\n",
       " '-0.229',\n",
       " '0.045921',\n",
       " '0.73164',\n",
       " '-0.23074']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r' ', line00.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('glove.6B.50d.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    for i, line in enumerate(fp):\n",
    "        if i == 0:\n",
    "            print(line)\n",
    "            print('\\n')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index2, index_to_word2, word_to_vec_map2 = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_vec_map2.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load dataset labeledTrainData.tsv and testData.tsv\n",
    "\n",
    "labeledTrainData = pd.read_csv (\"labeledTrainData.tsv\", sep = '\\t', header = [0])\n",
    "testData = pd.read_csv (\"testData.tsv\", sep = '\\t', header = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-23T23:54:55.182413Z",
     "iopub.status.busy": "2021-10-23T23:54:55.181645Z",
     "iopub.status.idle": "2021-10-23T23:55:04.556985Z",
     "shell.execute_reply": "2021-10-23T23:55:04.556158Z",
     "shell.execute_reply.started": "2021-10-23T23:54:55.182366Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% preprocessing, tokenisation and padding\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# for getting rid of html syntax\n",
    "# Import BeautifulSoup into your workspace\n",
    "from bs4 import BeautifulSoup    \n",
    "\n",
    "\"\"\"preprocess text by removing html syntax, keep only alphabets, whitespaces and fullstops, \n",
    "lowering case, and lemmatizing, removing stop words\"\"\"\n",
    "\n",
    "WNlemma_n = nltk.WordNetLemmatizer()\n",
    "\n",
    "def text_preprocess(text, pad = False, max_len=3000):\n",
    "    # remove html syntax\n",
    "    text1 = BeautifulSoup(text).get_text() \n",
    "    # get rid of unimportant punctuation marks\n",
    "    # !!! get rid of apostrophe too (and single quotation marks)\n",
    "#     text1 = re.sub(r'([^\\d\\w\\'\\s\\.\\-]+|[-\\.]{2,})', ' ', text1)\n",
    "    text1 = re.sub(r'([^\\d\\w\\s\\.\\-]+|[-\\.]{2,})', ' ', text1)\n",
    "    # only keep alphabets\n",
    "#     text1 = re.sub(r'[^a-zA-Z\\s]+', ' ', text1)\n",
    "    # lower case\n",
    "    text1 = text1.lower()\n",
    "    # lemmatise\n",
    "    text1 = WNlemma_n.lemmatize(text1)\n",
    "    # tokenise\n",
    "    text1 = nltk.word_tokenize(text1)\n",
    "    # remove stopwords\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     text1 = [word for word in text1 if not word in stop_words]\n",
    "    # text1 = (\" \").join(text1)\n",
    "    if pad == True:\n",
    "        # pad to max_len by '-1 empty'\n",
    "        if len(text1) < max_len:\n",
    "            text1 += ['-1 empty' for i in range(max_len - len(text1))]\n",
    "    return text1\n",
    "\n",
    "\n",
    "# def text_preprocess(text):\n",
    "#     # remove html syntax\n",
    "#     text1 = BeautifulSoup(text).get_text() \n",
    "#     # get rid of unimportant punctuation marks\n",
    "#     text1 = re.sub(r'([^\\d\\w\\'\\s\\.\\-]+|[-\\.]{2,})', ' ', text1)\n",
    "#     # lower case\n",
    "#     text1 = text1.lower()\n",
    "#     # lemmatise\n",
    "#     text1 = WNlemma_n.lemmatize(text1)\n",
    "#     return text1\n",
    "\n",
    "def pd_text_preprocess(row, pad = False, max_len=3000):\n",
    "    row['review_preprocessed'] = text_preprocess(row['review'], pad = pad, max_len = max_len)\n",
    "    row['length'] = len(row['review_preprocessed'])\n",
    "    return row\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledTrainData0 = labeledTrainData.copy()\n",
    "labeledTrainData0 = labeledTrainData0.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023920297622680664\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "import time\n",
    "start = time.time()\n",
    "labeledTrainData0 = labeledTrainData0.apply(pd_text_preprocess, axis = 1)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    25000.000000\n",
       "mean       244.449200\n",
       "std        180.219072\n",
       "min         11.000000\n",
       "25%        134.000000\n",
       "50%        183.000000\n",
       "75%        296.000000\n",
       "max       2600.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeledTrainData['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    25000.00000\n",
       "mean       238.93104\n",
       "std        175.03014\n",
       "min          6.00000\n",
       "25%        133.00000\n",
       "50%        181.00000\n",
       "75%        289.00000\n",
       "max       2325.00000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no padding\n",
    "max_len = 1500\n",
    "labeledTrainData = labeledTrainData.apply(pd_text_preprocess, axis = 1, pad = False, max_len = max_len)\n",
    "\n",
    "testData = testData.apply(pd_text_preprocess, axis = 1, pad = False, max_len = max_len)\n",
    "X_test = testData['review_preprocessed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stuff',\n",
       " 'going',\n",
       " 'moment',\n",
       " 'mj',\n",
       " 'started',\n",
       " 'listening',\n",
       " 'music',\n",
       " 'watching',\n",
       " 'odd',\n",
       " 'documentary',\n",
       " 'watched',\n",
       " 'wiz',\n",
       " 'watched',\n",
       " 'moonwalker',\n",
       " 'maybe',\n",
       " 'want',\n",
       " 'get',\n",
       " 'certain',\n",
       " 'insight',\n",
       " 'guy',\n",
       " 'thought',\n",
       " 'really',\n",
       " 'cool',\n",
       " 'eighties',\n",
       " 'maybe',\n",
       " 'make',\n",
       " 'mind',\n",
       " 'whether',\n",
       " 'guilty',\n",
       " 'innocent',\n",
       " 'moonwalker',\n",
       " 'part',\n",
       " 'biography',\n",
       " 'part',\n",
       " 'feature',\n",
       " 'film',\n",
       " 'remember',\n",
       " 'going',\n",
       " 'see',\n",
       " 'cinema',\n",
       " 'originally',\n",
       " 'released',\n",
       " 'subtle',\n",
       " 'messages',\n",
       " 'mj',\n",
       " 'feeling',\n",
       " 'towards',\n",
       " 'press',\n",
       " 'also',\n",
       " 'obvious',\n",
       " 'message',\n",
       " 'drugs',\n",
       " 'bad',\n",
       " 'kay',\n",
       " 'visually',\n",
       " 'impressive',\n",
       " 'course',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'unless',\n",
       " 'remotely',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'anyway',\n",
       " 'going',\n",
       " 'hate',\n",
       " 'find',\n",
       " 'boring',\n",
       " 'may',\n",
       " 'call',\n",
       " 'mj',\n",
       " 'egotist',\n",
       " 'consenting',\n",
       " 'making',\n",
       " 'movie',\n",
       " 'mj',\n",
       " 'fans',\n",
       " 'would',\n",
       " 'say',\n",
       " 'made',\n",
       " 'fans',\n",
       " 'true',\n",
       " 'really',\n",
       " 'nice',\n",
       " 'actual',\n",
       " 'feature',\n",
       " 'film',\n",
       " 'bit',\n",
       " 'finally',\n",
       " 'starts',\n",
       " 'minutes',\n",
       " 'excluding',\n",
       " 'smooth',\n",
       " 'criminal',\n",
       " 'sequence',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " 'convincing',\n",
       " 'psychopathic',\n",
       " 'powerful',\n",
       " 'drug',\n",
       " 'lord',\n",
       " 'wants',\n",
       " 'mj',\n",
       " 'dead',\n",
       " 'bad',\n",
       " 'beyond',\n",
       " 'mj',\n",
       " 'overheard',\n",
       " 'plans',\n",
       " 'nah',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " 'character',\n",
       " 'ranted',\n",
       " 'wanted',\n",
       " 'people',\n",
       " 'know',\n",
       " 'supplying',\n",
       " 'drugs',\n",
       " 'etc',\n",
       " 'dunno',\n",
       " 'maybe',\n",
       " 'hates',\n",
       " 'mj',\n",
       " 'music',\n",
       " 'lots',\n",
       " 'cool',\n",
       " 'things',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'turning',\n",
       " 'car',\n",
       " 'robot',\n",
       " 'whole',\n",
       " 'speed',\n",
       " 'demon',\n",
       " 'sequence',\n",
       " 'also',\n",
       " 'director',\n",
       " 'must',\n",
       " 'patience',\n",
       " 'saint',\n",
       " 'came',\n",
       " 'filming',\n",
       " 'kiddy',\n",
       " 'bad',\n",
       " 'sequence',\n",
       " 'usually',\n",
       " 'directors',\n",
       " 'hate',\n",
       " 'working',\n",
       " 'one',\n",
       " 'kid',\n",
       " 'let',\n",
       " 'alone',\n",
       " 'whole',\n",
       " 'bunch',\n",
       " 'performing',\n",
       " 'complex',\n",
       " 'dance',\n",
       " 'scene',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'movie',\n",
       " 'people',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'one',\n",
       " 'level',\n",
       " 'another',\n",
       " 'think',\n",
       " 'people',\n",
       " 'stay',\n",
       " 'away',\n",
       " 'try',\n",
       " 'give',\n",
       " 'wholesome',\n",
       " 'message',\n",
       " 'ironically',\n",
       " 'mj',\n",
       " 'bestest',\n",
       " 'buddy',\n",
       " 'movie',\n",
       " 'girl',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'truly',\n",
       " 'one',\n",
       " 'talented',\n",
       " 'people',\n",
       " 'ever',\n",
       " 'grace',\n",
       " 'planet',\n",
       " 'guilty',\n",
       " 'well',\n",
       " 'attention',\n",
       " 'gave',\n",
       " 'subject',\n",
       " 'hmmm',\n",
       " 'well',\n",
       " 'know',\n",
       " 'people',\n",
       " 'different',\n",
       " 'behind',\n",
       " 'closed',\n",
       " 'doors',\n",
       " 'know',\n",
       " 'fact',\n",
       " 'either',\n",
       " 'extremely',\n",
       " 'nice',\n",
       " 'stupid',\n",
       " 'guy',\n",
       " 'one',\n",
       " 'sickest',\n",
       " 'liars',\n",
       " 'hope',\n",
       " 'latter',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " '-1 empty',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeledTrainData['review_preprocessed'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(labeledTrainData['review_preprocessed'], labeledTrainData['sentiment'], random_state=0, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load embedding matrix pre-trained using glove\n",
    "\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "# read in the glove file. Each line in the text file is a string containing a word followed by the embedding vector, all separated by a whitespace\n",
    "# word_to_vec_map is a dict of words to their embedding vectors\n",
    "# (400,000 words, with the valid indices starting from 1 to 400,000\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        # starts from 1\n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "# index is valued  1-40000\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% function that converts training sentences into indices with padding\n",
    "\n",
    "# set unknown and padded tokens to 0; the embedding vectors will be zero-vectors\n",
    "padded_token_index = 0\n",
    "unknown_token_index = 0\n",
    "\n",
    "def sentences_to_indices(X, word_to_index):\n",
    "    \"\"\"\n",
    "    Converts an array of [padded, tokenised sentences] into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to 'Embedding()'\n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of padded, tokenised sentences, of shape (m, max_len)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    # set padded tokens to index = padded_token_index\n",
    "    word_to_index0 = word_to_index.copy()\n",
    "    word_to_index0['-1 empty'] = padded_token_index\n",
    "    \n",
    "    # set unknown tokens to index = unknown_token_index\n",
    "    def word_to_index00(key):\n",
    "        return word_to_index0.get(key, unknown_token_index)\n",
    "    \n",
    "    # speed up computation with vectorisation\n",
    "    X_indices = np.vectorize(word_to_index00)(X)\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train and X_valid are np.array of shape (m, max_len)\n",
    "X_train = np.array(X_train.tolist())\n",
    "X_valid = np.array(X_valid.tolist())\n",
    "y_train1 = np.expand_dims(y_train.to_numpy(), -1)\n",
    "y_valid1 = np.expand_dims(y_valid.to_numpy(), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(testData['review_preprocessed'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index)\n",
    "X_valid_indices = sentences_to_indices(X_valid, word_to_index)\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train_indices.npy', X_train_indices)\n",
    "np.save('X_valid_indices.npy', X_valid_indices)\n",
    "np.save('X_test_indices.npy', X_test_indices)\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('y_valid.npy', y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% baseline models with CountVectorizer, tfidf, ngrams, and logistic regression\n",
    "\"\"\"No embedding vector\"\"\"\n",
    "\n",
    "\"\"\"# CountVectorizer\"\"\"\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_count = CountVectorizer(min_df=0, max_df=0.2, stop_words='english', ngram_range = (1,5)).fit(X_train)\n",
    "# vect_count.get_feature_names()[::2000]\n",
    "X_train_count_vectorised = vect_count.transform(X_train)\n",
    "X_valid_count_vectorised = vect_count.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8833064220794353\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_count = LogisticRegression(max_iter = 10000)\n",
    "model_count.fit(X_train_count_vectorised, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions_count = model_count.predict(X_valid_count_vectorised)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('AUC: ', roc_auc_score(y_valid, predictions_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8758732819266494\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB_clf_count = MultinomialNB(alpha = 0.1)\n",
    "MNB_clf_count.fit(X_train_count_vectorised, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('AUC: ', roc_auc_score(y_valid, MNB_clf_count.predict(X_valid_count_vectorised)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "SVC_count = SVC()\n",
    "SVC_count.fit(X_train_count_vectorised, y_train)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('AUC: ', roc_auc_score(y_valid, SVC_count.predict(X_valid_count_vectorised)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit the TfidfVectorizer\n",
    "# minimum document frequency of 4 and max_df = 0.2, ignore stop words, ngram_range = (1,3)\n",
    "vect_tfidf = TfidfVectorizer(min_df=4, max_df=0.2, stop_words='english', ngram_range = (1,5)).fit(X_train)\n",
    "# vect_tfidf.get_feature_names()[::2000]\n",
    "X_train_tfidf_vectorised = vect_tfidf.transform(X_train)\n",
    "X_valid_tfidf_vectorised = vect_tfidf.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8911208227801097\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_tfidf = LogisticRegression(max_iter = 10000)\n",
    "model_tfidf.fit(X_train_tfidf_vectorised, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions_tfidf = model_tfidf.predict(vect_tfidf.transform(X_valid))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('AUC: ', roc_auc_score(y_valid, predictions_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
