# -*- coding: utf-8 -*-
"""
Created on Mon Nov 01 16:00:00 2021

@author: Cedric Yu
"""

"""
# From Sequence Model course on Coursera offered by DeepLearning.AI

# Named-Entity Recogniton to Process Resumes
# When faced with a large amount of unstructured text data, named-entity recognition (NER) can help you detect and classify important information in your dataset. For instance, in the running example "Jane vists Africa in September", NER would help you detect "Jane", "Africa", and "September" as named-entities and classify them as person, location, and time.

For the resume dataset, we:
# preprocess the dataset and extract target labels (the companies the applicant worked at, skills, type of degree...)
# tokenise texts and assign corresponding labels (entities) to tokens, or 'Empty' if no matching label
# use a Huggingface transformer: 
    # and its accompanied tokenizer and load pre-trained parameters and config from distilbert-base-cased
    # align labels: Transformer models are often trained by tokenizers that split words into subwords. align the lists of tags and the list of labels generated by the selected tokenizer 
    # re-train the model
# predict


Summary: 
    # 

"""

"""
This file:
For the resume dataset, we:
# preprocess the dataset and extract target labels (the companies the applicant worked at, skills, type of degree...)
# tokenise texts and assign corresponding labels (entities) to tokens, or 'Empty' if no matching label
# use a Huggingface transformer: 
    # and its accompanied tokenizer and load pre-trained parameters and config from distilbert-base-cased
    # align labels: Transformer models are often trained by tokenizers that split words into subwords. align the lists of tags and the list of labels generated by the selected tokenizer 
    # re-train the model
# predict

"""

# %% Preamble

# Make the output look better
from transformers import DistilBertTokenizerFast  # , TFDistilBertModel
import pandas as pd
import numpy as np
import seaborn as sn
import matplotlib.pyplot as plt
import json
import random
import logging
import re
import tensorflow as tf
import os
import pickle
from pickle import load
from seqeval.metrics import classification_report
from transformers import TFDistilBertForTokenClassification
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
# pd.set_option('display.width', 1000)
pd.set_option('display.max_colwidth', None)


os.chdir(r'C:\Users\Cedric Yu\Desktop\Works\10_nlp_named_entity_recognition')

# %% tensorflow

# import tensorflow as tf
# from keras.models import Model
# from keras.layers import Input, Dense, Reshape, merge
# from keras.layers.embeddings import Embedding
# from keras.preprocessing.sequence import skipgrams
# from keras.preprocessing import sequence

# from tensorflow.random import set_seed
# set_seed(0)
# np.random.seed(1)


# %% [labeled] datasets: functions for pre-processing and tokenisation

"""
Our labeled resume dataset is stored in 'resume/ner.json'. Each line is an instance. 
Each instance is a dictionary {"content": <content text string>, "annotation": [{"label":[<label>],"points":[{"start":<int>,"end":<int>,"text":<text>}] } ], ,"extras":null}. 

The values of "content" are the raw texts
"annotation" is a list containing the target labels (entities). In each element of the list, "label" is the label (entity, e.g. "Skills", "College Name") for the "text" which is the part of the "content". "start" and "end" denote the starting and ending characters of the "text" in "content".
"extras": not used.

Example: line 1 reads
{"content": "Abhishek Jha\nApplication Development Associate...","annotation":[{"label":["Skills"],"points":[{"start":1295,"end":1621,"text":"\nâ€¢ Programming language: C, C++, Java\nâ€¢ Oracle PeopleSoft\nâ€¢ ..."}]},...],"extras":null}

"""

"""
This section is for pre-processing [labeled] datasets
"""

"""
For [labeled] datasets:
convert json into spaCy format: a list of (content, {"entities" : entities}), where entities is a list of (point_start, point_end + 1 , label)
# first, replace '\n' by ' '
# then, note that in 'text' of each annotation, there are trailing spaces. Correct (start, end) by excluding the trailing spaces in 'text'
"""


def convert_dataturks_to_spacy(dataturks_JSON_FilePath):
    try:
        training_data = []
        lines = []
        with open(dataturks_JSON_FilePath, 'r', encoding="utf8") as f:
            lines = f.readlines()

        # each line is one instance
        for line in lines:
            data = json.loads(line)
            # replace line break '\n' by ' '
            content = data['content'].replace("\n", " ")
            # collect all entity labels from 'annotation'
            entities = []
            data_annotations = data['annotation']
            if data_annotations is not None:
                for annotation in data_annotations:
                    # only a single point in text annotation.
                    point = annotation['points'][0]
                    labels = annotation['label']
                    # handle both [list of labels] or [a single label]
                    if not isinstance(labels, list):
                        labels = [labels]

                    # in 'point', strip left and right whitespaces in 'text' and update 'start' and 'end' positions ignoring the spaces
                    for label in labels:
                        point_start = point['start']
                        point_end = point['end']
                        point_text = point['text']

                        lstrip_diff = len(point_text) - \
                            len(point_text.lstrip())
                        rstrip_diff = len(point_text) - \
                            len(point_text.rstrip())
                        if lstrip_diff != 0:
                            point_start = point_start + lstrip_diff
                        if rstrip_diff != 0:
                            point_end = point_end - rstrip_diff
                        entities.append((point_start, point_end + 1, label))
            training_data.append((content, {"entities": entities}))
        return training_data

    except Exception as e:
        logging.exception("Unable to process " +
                          dataturks_JSON_FilePath + "\n" + "error = " + str(e))
        return None


# the 'data:list' and '->' are just annotations that specify input and return dtypes
"""
For [labeled] datasets:
# we have corrected (start, end) by disregarding trailing spaces from 'text' of each entity. now we correct (start, end) of each entity by checking that they do not correspond to trailing whitespaces in 'content'
"""


def trim_entity_spans(data: list) -> list:
    """Removes leading and trailing white spaces from entity spans.

    Args:
        data (list): The data to be cleaned in spaCy JSON format.

    Returns:
        list: The cleaned data.
    """
    invalid_span_tokens = re.compile(r'\s')

    cleaned_data = []
    for content, annotations in data:
        entities = annotations['entities']
        valid_entities = []
        for start, end, label in entities:
            valid_start = start
            valid_end = end
            while valid_start < len(content) and invalid_span_tokens.match(
                    content[valid_start]):
                valid_start += 1
            while valid_end > 1 and invalid_span_tokens.match(
                    content[valid_end - 1]):
                valid_end -= 1
            valid_entities.append([valid_start, valid_end, label])
        cleaned_data.append([content, {'entities': valid_entities}])
    return cleaned_data


"""
For [labeled] datasets:
# tokenise content by '[\s]+' or '\W'
# only keep non-whitespace tokens
# assign corresponding labels (entities) to tokens, or 'Empty' if no matching label
"""


def tokentize_dataset_tags(data):
    content_tokens = []
    labels = []

    for m in range(len(data)):

        content = data[m][0]
        entitiesm = data[m][1]

        # split content by '[\s]+' or '\W', discard empty strings
        token_split = list(
            filter(None, re.split(r'(\W\b|\b\W|[\s]+)', content)))
        # content_spaces = [(m.start(0), m.end(0)) for m in re.finditer(r'[\s]+', content)]

        token_index = []
        token_split_no_space = []
        char_index = 0
        for token in token_split:
            start = char_index
            end = char_index + len(token)
            # discard whitespace tokens
            if len(re.findall(r'[\s]+', token)) != 1:
                token_index.append((token, start, end))
                token_split_no_space.append(token)

            char_index = end

        # assign corresponding labels (entities) to tokens, or 'Empty' if no matching label
        emptyList = ["Empty"] * len(token_index)
        for j, token in enumerate(token_index):
            for entity in entitiesm['entities']:
                if (token[1] >= entity[0]) & (token[2] <= entity[1]):
                    emptyList[j] = entity[2]

        content_tokens.append(token_split_no_space)
        labels.append(emptyList)

    return content_tokens, labels


"""example (first training instance with np random seed(1) given below"""

data0 = trim_entity_spans([convert_dataturks_to_spacy("resume/ner.json")[37]])
# [['Kowsick Somasundaram Certified Network Associate Training Program  Erode, Tamil Nadu - Email me on Indeed: indeed.com/r/Kowsick- Somasundaram/3bd9e5de546cc3c8  Bachelor of computer science graduate seeking opportunities in the field of ITIS to contribute to corporate goals and objectives. Easily adapt to changes, with eagerness toward learning and expanding capabilities.  EXPERIENCE:-  WORK EXPERIENCE  Certified Network Associate Training Program  Cisco -  July 2013 to October 2013  â€¢ Workshop on computer Hardware& Software.  â€¢ Workshop on Web development.  EDUCATION  Bachelor of computer science in computer science  inDR N.G.P ARTS AND SCIENCE COLLEGE -  Coimbatore, Tamil Nadu  SKILLS  DHCP (Less than 1 year), DNS (Less than 1 year), EXCHANGE (Less than 1 year), exchange (Less than 1 year), LAN (Less than 1 year)  ADDITIONAL INFORMATION  SKILLS:-  â€¢ Messaging: MS exchange, Lotus client and MS outlook issue coordination to user.  â€¢ Users / Share folders creation and permission assigning.  â€¢ Networking: TCP/IP, DNS, DHCP, and LAN/WAN.  â€¢ Monthly patching update activity and server owner approval / RFC follow-ups.  https://www.indeed.com/r/Kowsick-Somasundaram/3bd9e5de546cc3c8?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Kowsick-Somasundaram/3bd9e5de546cc3c8?isid=rex-download&ikw=download-top&co=IN',
#   {'entities': [[696, 1129, 'Skills'],
#     [625, 660, 'College Name'],
#     [575, 623, 'Degree'],
#     [452, 457, 'Companies worked at'],
#     [406, 450, 'Designation'],
#     [107, 158, 'Email Address'],
#     [67, 72, 'Location'],
#     [21, 65, 'Designation'],
#     [0, 20, 'Name']]}]]

content0 = data0[0][0]
content0[0:20]
# 'Kowsick Somasundaram'
content0[21:65]
# 'Certified Network Associate Training Program'
content0[67:72]
# 'Erode'
content0[452:457]
# 'Cisco'
content0[575:623]
# 'Bachelor of computer science in computer science'
content0[625:660]
# 'inDR N.G.P ARTS AND SCIENCE COLLEGE'
content0[696:1129]
# 'DHCP (Less than 1 year), DNS (Less than 1 year), EXCHANGE (Less than 1 year), exchange (Less than 1 year), LAN (Less than 1 year)  ADDITIONAL INFORMATION  SKILLS:-  â€¢ Messaging: MS exchange, Lotus client and MS outlook issue coordination to user.  â€¢ Users / Share folders creation and permission assigning.  â€¢ Networking: TCP/IP, DNS, DHCP, and LAN/WAN.  â€¢ Monthly patching update activity and server owner approval / RFC follow-ups.'

# entities0 = data0[0][1]
# {'entities': [[696, 1129, 'Skills'],
#   [625, 660, 'College Name'],
#   [575, 623, 'Degree'],
#   [452, 457, 'Companies worked at'],
#   [406, 450, 'Designation'],
#   [107, 158, 'Email Address'],
#   [67, 72, 'Location'],
#   [21, 65, 'Designation'],
#   [0, 20, 'Name']]}

token_split0 = list(filter(None, re.split(r'(\W\b|\b\W|[\s]+)', content0)))
# ['Kowsick', ' ', 'Somasundaram', ' ', 'Certified', ' ', 'Network', ' ', 'Associate', ' ', 'Training', ' ', 'Program', ' ', ' ', 'Erode', ',', ' ', 'Tamil', ' ', 'Nadu', ' ', '-', ' ', 'Email', ' ', 'me', ' ', 'on', ' ', 'Indeed', ':', ' ', 'indeed', '.', 'com', '/', 'r', '/', 'Kowsick', '-', ' ', 'Somasundaram', '/', '3bd9e5de546cc3c8', ' ', ' ', 'Bachelor', ' ', 'of', ' ', 'computer', ' ', 'science', ' ', 'graduate', ' ', 'seeking', ' ', 'opportunities', ' ', 'in', ' ', 'the', ' ', 'field', ' ', 'of', ' ', 'ITIS', ' ', 'to', ' ', 'contribute', ' ', 'to', ' ', 'corporate', ' ', 'goals', ' ', 'and', ' ', 'objectives', '.', ' ', 'Easily', ' ', 'adapt', ' ', 'to', ' ', 'changes', ',', ' ', 'with', ' ', 'eagerness', ' ', 'toward', ' ', 'learning', ' ', 'and', ' ', 'expanding', ' ', 'capabilities', '.', '  ', 'EXPERIENCE', ':', '-', '  ', 'WORK', ' ', 'EXPERIENCE', ' ', ' ', 'Certified', ' ', 'Network', ' ', 'Associate', ' ', 'Training', ' ', 'Program', ' ', ' ', 'Cisco', ' ', '-', '  ', 'July', ' ', '2013', ' ', 'to', ' ', 'October', ' ', '2013', ' ', ' ', 'â€¢', ' ', 'Workshop', ' ', 'on', ' ', 'computer', ' ', 'Hardware', '&', ' ', 'Software', '.', '  ', 'â€¢', ' ', 'Workshop', ' ', 'on', ' ', 'Web', ' ', 'development', '.', '  ', 'EDUCATION', ' ', ' ', 'Bachelor', ' ', 'of', ' ', 'computer', ' ', 'science', ' ', 'in', ' ', 'computer', ' ', 'science', ' ', ' ', 'inDR', ' ', 'N', '.', 'G', '.', 'P', ' ', 'ARTS', ' ', 'AND', ' ', 'SCIENCE', ' ', 'COLLEGE', ' ', '-', '  ', 'Coimbatore', ',', ' ', 'Tamil', ' ', 'Nadu', ' ', ' ', 'SKILLS', ' ', ' ', 'DHCP', ' ', '(', 'Less', ' ', 'than', ' ', '1', ' ', 'year', ')', ',', ' ', 'DNS', ' ', '(', 'Less', ' ', 'than', ' ', '1', ' ', 'year', ')', ',', ' ', 'EXCHANGE', ' ', '(', 'Less', ' ', 'than', ' ', '1', ' ', 'year', ')', ',', ' ', 'exchange', ' ', '(', 'Less', ' ', 'than', ' ', '1', ' ', 'year', ')', ',', ' ', 'LAN', ' ', '(', 'Less', ' ', 'than', ' ', '1', ' ', 'year', ')', '  ', 'ADDITIONAL', ' ', 'INFORMATION', ' ', ' ', 'SKILLS', ':', '-', '  ', 'â€¢', ' ', 'Messaging', ':', ' ', 'MS', ' ', 'exchange', ',', ' ', 'Lotus', ' ', 'client', ' ', 'and', ' ', 'MS', ' ', 'outlook', ' ', 'issue', ' ', 'coordination', ' ', 'to', ' ', 'user', '.', '  ', 'â€¢', ' ', 'Users', ' ', '/', ' ', 'Share', ' ', 'folders', ' ', 'creation', ' ', 'and', ' ', 'permission', ' ', 'assigning', '.', '  ', 'â€¢', ' ', 'Networking', ':', ' ', 'TCP', '/', 'IP', ',', ' ', 'DNS', ',', ' ', 'DHCP', ',', ' ', 'and', ' ', 'LAN', '/', 'WAN', '.', '  ', 'â€¢', ' ', 'Monthly', ' ', 'patching', ' ', 'update', ' ', 'activity', ' ', 'and', ' ', 'server', ' ', 'owner', ' ', 'approval', ' ', '/', ' ', 'RFC', ' ', 'follow', '-', 'ups', '.', '  ', 'https', ':', '/', '/', 'www', '.', 'indeed', '.', 'com', '/', 'r', '/', 'Kowsick', '-', 'Somasundaram', '/', '3bd9e5de546cc3c8', '?', 'isid', '=', 'rex', '-', 'download', '&', 'ikw', '=', 'download', '-', 'top', '&', 'co', '=', 'IN', ' ', 'https', ':', '/', '/', 'www', '.', 'indeed', '.', 'com', '/', 'r', '/', 'Kowsick', '-', 'Somasundaram', '/', '3bd9e5de546cc3c8', '?', 'isid', '=', 'rex', '-', 'download', '&', 'ikw', '=', 'download', '-', 'top', '&', 'co', '=', 'IN']


token_split_no_space0, labels0 = tokentize_dataset_tags(data0)
print(list(zip(token_split_no_space0[0], labels0[0])))
# correct.
# [('Kowsick', 'Name'), ('Somasundaram', 'Name'), ('Certified', 'Designation'), ('Network', 'Designation'), ('Associate', 'Designation'), ('Training', 'Designation'), ('Program', 'Designation'), ('Erode', 'Location'), (',', 'Empty'), ('Tamil', 'Empty'), ('Nadu', 'Empty'), ('-', 'Empty'), ('Email', 'Empty'), ('me', 'Empty'), ('on', 'Empty'), ('Indeed', 'Empty'), (':', 'Empty'), ('indeed', 'Email Address'), ('.', 'Email Address'), ('com', 'Email Address'), ('/', 'Email Address'), ('r', 'Email Address'), ('/', 'Email Address'), ('Kowsick', 'Email Address'), ('-', 'Email Address'), ('Somasundaram', 'Email Address'), ('/', 'Email Address'), ('3bd9e5de546cc3c8', 'Email Address'), ('Bachelor', 'Empty'), ('of', 'Empty'), ('computer', 'Empty'), ('science', 'Empty'), ('graduate', 'Empty'), ('seeking', 'Empty'), ('opportunities', 'Empty'), ('in', 'Empty'), ('the', 'Empty'), ('field', 'Empty'), ('of', 'Empty'), ('ITIS', 'Empty'), ('to', 'Empty'), ('contribute', 'Empty'), ('to', 'Empty'), ('corporate', 'Empty'), ('goals', 'Empty'), ('and', 'Empty'), ('objectives', 'Empty'), ('.', 'Empty'), ('Easily', 'Empty'), ('adapt', 'Empty'), ('to', 'Empty'), ('changes', 'Empty'), (',', 'Empty'), ('with', 'Empty'), ('eagerness', 'Empty'), ('toward', 'Empty'), ('learning', 'Empty'), ('and', 'Empty'), ('expanding', 'Empty'), ('capabilities', 'Empty'), ('.', 'Empty'), ('EXPERIENCE', 'Empty'), (':', 'Empty'), ('-', 'Empty'), ('WORK', 'Empty'), ('EXPERIENCE', 'Empty'), ('Certified', 'Designation'), ('Network', 'Designation'), ('Associate', 'Designation'), ('Training', 'Designation'), ('Program', 'Designation'), ('Cisco', 'Companies worked at'), ('-', 'Empty'), ('July', 'Empty'), ('2013', 'Empty'), ('to', 'Empty'), ('October', 'Empty'), ('2013', 'Empty'), ('â€¢', 'Empty'), ('Workshop', 'Empty'), ('on', 'Empty'), ('computer', 'Empty'), ('Hardware', 'Empty'), ('&', 'Empty'), ('Software', 'Empty'), ('.', 'Empty'), ('â€¢', 'Empty'), ('Workshop', 'Empty'), ('on', 'Empty'), ('Web', 'Empty'), ('development', 'Empty'), ('.', 'Empty'), ('EDUCATION', 'Empty'), ('Bachelor', 'Degree'), ('of', 'Degree'), ('computer', 'Degree'), ('science', 'Degree'), ('in', 'Degree'), ('computer', 'Degree'), ('science', 'Degree'), ('inDR', 'College Name'), ('N', 'College Name'), ('.', 'College Name'), ('G', 'College Name'), ('.', 'College Name'), ('P', 'College Name'), ('ARTS', 'College Name'), ('AND', 'College Name'), ('SCIENCE', 'College Name'), ('COLLEGE', 'College Name'), ('-', 'Empty'), ('Coimbatore', 'Empty'), (',', 'Empty'), ('Tamil', 'Empty'), ('Nadu', 'Empty'), ('SKILLS', 'Empty'), ('DHCP', 'Skills'), ('(', 'Skills'), ('Less', 'Skills'), ('than', 'Skills'), ('1', 'Skills'), ('year', 'Skills'), (')', 'Skills'), (',', 'Skills'), ('DNS', 'Skills'), ('(', 'Skills'), ('Less', 'Skills'), ('than', 'Skills'), ('1', 'Skills'), ('year', 'Skills'), (')', 'Skills'), (',', 'Skills'), ('EXCHANGE', 'Skills'), ('(', 'Skills'), ('Less', 'Skills'), ('than', 'Skills'), ('1', 'Skills'), ('year', 'Skills'), (')', 'Skills'), (',', 'Skills'), ('exchange', 'Skills'), ('(', 'Skills'), ('Less', 'Skills'), ('than', 'Skills'), ('1', 'Skills'), ('year', 'Skills'), (')', 'Skills'), (',', 'Skills'), ('LAN', 'Skills'), ('(', 'Skills'), ('Less', 'Skills'), ('than', 'Skills'), ('1', 'Skills'), ('year', 'Skills'), (')', 'Skills'), ('ADDITIONAL', 'Skills'), ('INFORMATION', 'Skills'), ('SKILLS', 'Skills'), (':', 'Skills'), ('-', 'Skills'), ('â€¢', 'Skills'), ('Messaging', 'Skills'), (':', 'Skills'), ('MS', 'Skills'), ('exchange', 'Skills'), (',', 'Skills'), ('Lotus', 'Skills'), ('client', 'Skills'), ('and', 'Skills'), ('MS', 'Skills'), ('outlook', 'Skills'), ('issue', 'Skills'), ('coordination', 'Skills'), ('to', 'Skills'), ('user', 'Skills'), ('.', 'Skills'), ('â€¢', 'Skills'), ('Users', 'Skills'), ('/', 'Skills'), ('Share', 'Skills'), ('folders', 'Skills'), ('creation', 'Skills'), ('and', 'Skills'), ('permission', 'Skills'), ('assigning', 'Skills'), ('.', 'Skills'), ('â€¢', 'Skills'), ('Networking', 'Skills'), (':', 'Skills'), ('TCP', 'Skills'), ('/', 'Skills'), ('IP', 'Skills'), (',', 'Skills'), ('DNS', 'Skills'), (',', 'Skills'), ('DHCP', 'Skills'), (',', 'Skills'), ('and', 'Skills'), ('LAN', 'Skills'), ('/', 'Skills'), ('WAN', 'Skills'), ('.', 'Skills'), ('â€¢', 'Skills'), ('Monthly', 'Skills'), ('patching', 'Skills'), ('update', 'Skills'), ('activity', 'Skills'), ('and', 'Skills'), ('server', 'Skills'), ('owner', 'Skills'), ('approval', 'Skills'), ('/', 'Skills'), ('RFC', 'Skills'), ('follow', 'Skills'), ('-', 'Skills'), ('ups', 'Skills'), ('.', 'Skills'), ('https', 'Empty'), (':', 'Empty'), ('/', 'Empty'), ('/', 'Empty'), ('www', 'Empty'), ('.', 'Empty'), ('indeed', 'Empty'), ('.', 'Empty'), ('com', 'Empty'), ('/', 'Empty'), ('r', 'Empty'), ('/', 'Empty'), ('Kowsick', 'Empty'), ('-', 'Empty'), ('Somasundaram', 'Empty'), ('/', 'Empty'), ('3bd9e5de546cc3c8', 'Empty'), ('?', 'Empty'), ('isid', 'Empty'), ('=', 'Empty'), ('rex', 'Empty'), ('-', 'Empty'), ('download', 'Empty'), ('&', 'Empty'), ('ikw', 'Empty'), ('=', 'Empty'), ('download', 'Empty'), ('-', 'Empty'), ('top', 'Empty'), ('&', 'Empty'), ('co', 'Empty'), ('=', 'Empty'), ('IN', 'Empty'), ('https', 'Empty'), (':', 'Empty'), ('/', 'Empty'), ('/', 'Empty'), ('www', 'Empty'), ('.', 'Empty'), ('indeed', 'Empty'), ('.', 'Empty'), ('com', 'Empty'), ('/', 'Empty'), ('r', 'Empty'), ('/', 'Empty'), ('Kowsick', 'Empty'), ('-', 'Empty'), ('Somasundaram', 'Empty'), ('/', 'Empty'), ('3bd9e5de546cc3c8', 'Empty'), ('?', 'Empty'), ('isid', 'Empty'), ('=', 'Empty'), ('rex', 'Empty'), ('-', 'Empty'), ('download', 'Empty'), ('&', 'Empty'), ('ikw', 'Empty'), ('=', 'Empty'), ('download', 'Empty'), ('-', 'Empty'), ('top', 'Empty'), ('&', 'Empty'), ('co', 'Empty'), ('=', 'Empty'), ('IN', 'Empty')]


# %% load labeled dataset. train-validation split and pre-processing

data = trim_entity_spans(convert_dataturks_to_spacy("resume/ner.json"))
len(data)
# 220
# token_split_no_space, labels = tokentize_dataset_tags(data)

"""train-validation split: 7-3"""
np.random.seed(1)
train_ind = [np.random.randint(len(data)) for i in range(int(len(data)*0.7))]
valid_ind = [np.random.randint(len(data))
             for i in range(len(data) - int(len(data)*0.7))]

data_train = [data[i] for i in train_ind]
data_valid = [data[i] for i in valid_ind]

content_train = [content for content, entities in data_train]
content_valid = [content for content, entities in data_valid]

token_split_no_space_train, labels_train = tokentize_dataset_tags(data_train)
token_split_no_space_valid, labels_valid = tokentize_dataset_tags(data_valid)


# %% Resume: unlabeled dataset tokenisation

"""
Assume our unlabeled test set is a line-separated json file
"""


def convert_dataturks_to_spacy_unlabeled(dataturks_JSON_FilePath):
    try:
        test_data = []
        lines = []
        with open(dataturks_JSON_FilePath, 'r', encoding="utf8") as f:
            lines = f.readlines()

        # each line is one instance
        for line in lines:
            data = json.loads(line)
            # replace line break '\n' by ' '
            content = data['content'].replace("\n", " ")
            test_data.append(content)
        return test_data

    except Exception as e:
        logging.exception("Unable to process " +
                          dataturks_JSON_FilePath + "\n" + "error = " + str(e))
        return None
# returns a list of strings


def tokentize_dataset(data):
    """
    data is a list of m strings with '\n' replaced by ' '
    """

    content_tokens = []

    for m in range(len(data)):

        content = data[m]

        token_split = list(
            filter(None, re.split(r'(\W\b|\b\W|[\s]+)', content)))
        # content_spaces = [(m.start(0), m.end(0)) for m in re.finditer(r'[\s]+', content)]

        token_index = []
        token_split_no_space = []
        char_index = 0
        for token in token_split:
            start = char_index
            end = char_index + len(token)
            if len(re.findall(r'[\s]+', token)) != 1:
                token_index.append((token, start, end))
                token_split_no_space.append(token)

            char_index = end

        content_tokens.append(token_split_no_space)

    return content_tokens


# %% tags from labeled training set

"""generate a list of unique tags to which the named-entities will be matched"""
"""create id for each unique tag and dictionary between tags and ids"""

unique_tags = set(pd.Series(labels_train).explode().unique())
tag2id = {tag: id for id, tag in enumerate(unique_tags)}
id2tag = {id: tag for tag, id in tag2id.items()}

# unique_tags
# {'College Name',
#  'Companies worked at',
#  'Degree',
#  'Designation',
#  'Email Address',
#  'Empty',
#  'Graduation Year',
#  'Location',
#  'Name',
#  'Skills',
#  'UNKNOWN',
#  'Years of Experience'}

# pd.Series(labels_train).explode().value_counts()
# Empty                  85440
# Skills                  8273
# Email Address           1930
# Designation             1015
# Degree                   938
# College Name             889
# Companies worked at      766
# Location                 362
# Name                     335
# Graduation Year          161
# Years of Experience       98
# UNKNOWN                    7
# dtype: int64
# obviously most are 'Empty'; beware of class imbalance

"""save to files"""


# create a binary pickle file
f = open("tag2id.pkl", "wb")
# write the python object to pickle file
pickle.dump(tag2id, f)
# close file
f.close()

# create a binary pickle file
f = open("id2tag.pkl", "wb")
# write the python object (dict) to pickle file
pickle.dump(id2tag, f)
# close file
f.close()


tag2id = load(open('tag2id.pkl', 'rb'))
id2tag = load(open('id2tag.pkl', 'rb'))


# %% convert labels to ids (aka tags)

# from tensorflow.keras.preprocessing.sequence import pad_sequences
# MAX_LEN = 512

# len(labels_train)
# 154

# convert labels to ids
tags_train = [[tag2id.get(l) for l in lab] for lab in labels_train]
# pad_sequences([[tag2id.get(l) for l in lab] for lab in labels_train],
#                      maxlen=MAX_LEN, value=tag2id["Empty"], padding="post",
#                      dtype="long", truncating="post")
tags_valid = [[tag2id.get(l) for l in lab] for lab in labels_valid]
# they will be further tokenised and padded by Huggingface tokenizer


# %% tokenisation and label (tag id) alignment with the huggingface library

"""
Before feeding the texts to a Transformer model, we will need to tokenize our input using a huggingface Transformer tokenizer. 
!!! It is crucial that the tokenizer we use must match the Transformer model type you are using
we will use the huggingface DistilBERT fast tokenizer, which standardizes the length of our sequence to 512 and pads with zeros. Notice this matches the maximum length we used when creating tags.
"""


"""
Transformer models are often trained by tokenizers that split words into subwords. 
For instance, the word 'Africa' might get split into multiple subtokens. 
This can create some misalignment between [the list of tags from the labeled dataset] and [the list of labels generated by the tokenizer], since the tokenizer can split one word into several, or add special tokens. 
Before processing, it is important that we align the lists of tags and the list of labels generated by the selected tokenizer with a tokenize_and_align_labels() function.
"""

"""
Align the list of tags and labels with the tokenizer word_ids method, return a list that maps the subtokens to the original word in the sentence and special tokens to None.
Set the labels of all the special tokens (None) to -100 to prevent them from affecting the loss function.
Label of the first subtoken of a word and set the label for the following subtokens to -100.
"""


"""
let us look at an example to gain intuition: examples="Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka"
"""

# from transformers import DistilBertTokenizerFast #, TFDistilBertModel
# tokenizer = DistilBertTokenizerFast.from_pretrained('pre-trained-transformer-distilbert-base-cased/')

# # the huggingface tokeniser tokenises it into:
# np.array(tokenizer.tokenize(df_data0['content'].values.tolist()))
# # ['ab', '##his', '##he', '##k', 'j', '##ha', 'application',
#        # 'development', 'associate', '-', 'accent', '##ure', ...]
# """# so it splits words into subwords"""

# each subword has an 'input_id'
# tokenized_inputs0 = tokenizer(df_data0['content'].values.tolist(), truncation=True, is_split_into_words=False, padding='max_length', max_length=512)
# # {'input_ids': [[101, 11113, 24158, 5369, 2243, 1046, 3270, 4646, 2458, 5482, 1011, 9669, 5397, 8191, 14129,...

# while the tag ids from our labeled dataset are
# tags0
# # array([[9, 9, 2, ..., 1, 1, 1],
# #        [9, 9, 1, ..., 1, 1, 1]])

# tokenized_inputs0.word_ids(batch_index=0)
# np.array(tokenized_inputs0.word_ids(batch_index=0))
# # array([None, 0, 0, 0, 0, 1, 1, 2, 3, 4, 5, 6, 6, 7, 7, 8, 9, 10, 11, 12,
# #        13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 22, 22, 22, 23, 24, 24, 25,
# #        26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 28, 29, 30, 31, 32,
# #        33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,
# #        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,
# #        67, 68, 69, 70, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,
# """ word_ids assigns, in ascending order, the same word id for all sub-words of the same original word"""


""" another example (first training set instance)"""

text0 = data[37][0]
# 'Kowsick Somasundaram Certified Network Associate Training Program  Erode, Tamil Nadu - Email me on Indeed: indeed.com/r/Kowsick- Somasundaram/3bd9e5de546cc3c8  Bachelor of computer science graduate seeking opportunities in the field of ITIS to contribute to corporate goals and objectives. Easily adapt to changes, with eagerness toward learning and expanding capabilities.  EXPERIENCE:-  WORK EXPERIENCE  Certified Network Associate Training Program  Cisco -  July 2013 to October 2013  â€¢ Workshop on computer Hardware& Software.  â€¢ Workshop on Web development.  EDUCATION  Bachelor of computer science in computer science  inDR N.G.P ARTS AND SCIENCE COLLEGE -  Coimbatore, Tamil Nadu  SKILLS  DHCP (Less than 1 year), DNS (Less than 1 year), EXCHANGE (Less than 1 year), exchange (Less than 1 year), LAN (Less than 1 year)  ADDITIONAL INFORMATION  SKILLS:-  â€¢ Messaging: MS exchange, Lotus client and MS outlook issue coordination to user.  â€¢ Users / Share folders creation and permission assigning.  â€¢ Networking: TCP/IP, DNS, DHCP, and LAN/WAN.  â€¢ Monthly patching update activity and server owner approval / RFC follow-ups.  https://www.indeed.com/r/Kowsick-Somasundaram/3bd9e5de546cc3c8?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Kowsick-Somasundaram/3bd9e5de546cc3c8?isid=rex-download&ikw=download-top&co=IN'

# print(tokenizer.tokenize(token_split_no_space_train[0], is_split_into_words=True))
# ['Ko', '##ws', '##ick', 'So', '##mas', '##unda', '##ram', 'Certified', 'Network', 'Associate', 'Training', 'Program', 'E', '##rod', '##e', ',', 'Tamil', 'Nadu', '-', 'Em', '##ail', 'me', 'on', 'Indeed', ':', 'indeed', '.', 'com', '/', 'r', '/', 'Ko', '##ws', '##ick', '-', 'So', '##mas', '##unda', '##ram', '/', '3', '##b', '##d', '##9', '##e', '##5', '##de', '##5', '##46', '##cc', '##3', '##c', '##8', 'Bachelor', 'of', 'computer', 'science', 'graduate', 'seeking', 'opportunities', 'in', 'the', 'field', 'of', 'IT', '##IS', 'to', 'contribute', 'to', 'corporate', 'goals', 'and', 'objectives', '.', 'E', '##asily', 'adapt', 'to', 'changes', ',', 'with', 'eager', '##ness', 'toward', 'learning', 'and', 'expanding', 'capabilities', '.', 'E', '##X', '##P', '##ER', '##IE', '##NC', '##E', ':', '-', 'W', '##OR', '##K', 'E', '##X', '##P', '##ER', '##IE', '##NC', '##E', 'Certified', 'Network', 'Associate', 'Training', 'Program', 'C', '##isco', '-', 'July', '2013', 'to', 'October', '2013', 'â€¢', 'Workshop', 'on', 'computer', 'Hard', '##ware', '&', 'Software', '.', 'â€¢', 'Workshop', 'on', 'Web', 'development', '.', 'E', '##D', '##UC', '##AT', '##ION', 'Bachelor', 'of', 'computer', 'science', 'in', 'computer', 'science', 'in', '##DR', 'N', '.', 'G', '.', 'P', 'AR', '##TS', 'AND', 'SC', '##IE', '##NC', '##E', 'CO', '##LL', '##EG', '##E', '-', 'Co', '##im', '##bat', '##ore', ',', 'Tamil', 'Nadu', 'SK', '##IL', '##LS', 'D', '##HC', '##P', '(', 'Less', 'than', '1', 'year', ')', ',', 'D', '##NS', '(', 'Less', 'than', '1', 'year', ')', ',', 'E', '##X', '##CH', '##AN', '##GE', '(', 'Less', 'than', '1', 'year', ')', ',', 'exchange', '(', 'Less', 'than', '1', 'year', ')', ',', 'LA', '##N', '(', 'Less', 'than', '1', 'year', ')', 'AD', '##DI', '##TI', '##ON', '##AL', 'IN', '##F', '##OR', '##MA', '##TI', '##ON', 'SK', '##IL', '##LS', ':', '-', 'â€¢', 'Me', '##ssa', '##ging', ':', 'MS', 'exchange', ',', 'Lotus', 'client', 'and', 'MS', 'outlook', 'issue', 'coordination', 'to', 'user', '.', 'â€¢', 'Users', '/', 'S', '##hare', 'folder', '##s', 'creation', 'and', 'permission', 'assign', '##ing', '.', 'â€¢', 'Network', '##ing', ':', 'T', '##CP', '/', 'IP', ',', 'D', '##NS', ',', 'D', '##HC', '##P', ',', 'and', 'LA', '##N', '/', 'WA', '##N', '.', 'â€¢', 'Monthly', 'patch', '##ing', 'update', 'activity', 'and', 'server', 'owner', 'approval', '/', 'RFC', 'follow', '-', 'ups', '.', 'https', ':', '/', '/', 'www', '.', 'indeed', '.', 'com', '/', 'r', '/', 'Ko', '##ws', '##ick', '-', 'So', '##mas', '##unda', '##ram', '/', '3', '##b', '##d', '##9', '##e', '##5', '##de', '##5', '##46', '##cc', '##3', '##c', '##8', '?', 'is', '##id', '=', 're', '##x', '-', 'download', '&', 'i', '##k', '##w', '=', 'download', '-', 'top', '&', 'co', '=', 'IN', 'https', ':', '/', '/', 'www', '.', 'indeed', '.', 'com', '/', 'r', '/', 'Ko', '##ws', '##ick', '-', 'So', '##mas', '##unda', '##ram', '/', '3', '##b', '##d', '##9', '##e', '##5', '##de', '##5', '##46', '##cc', '##3', '##c', '##8', '?', 'is', '##id', '=', 're', '##x', '-', 'download', '&', 'i', '##k', '##w', '=', 'download', '-', 'top', '&', 'co', '=', 'IN']

# tokenized_inputs0 = tokenizer([token_split_no_space_train[0]], is_split_into_words=True)
# print(tokenized_inputs0['input_ids'])
# [[101, 19892, 10732, 5345, 1573, 7941, 22902, 4515, 25777, 3998, 9666, 5513, 4659, 142, 13225, 1162, 117, 5344, 10657, 118, 18653, 11922, 1143, 1113, 10364, 131, 5750, 119, 3254, 120, 187, 120, 19892, 10732, 5345, 118, 1573, 7941, 22902, 4515, 120, 124, 1830, 1181, 1580, 1162, 1571, 2007, 1571, 23435, 19515, 1495, 1665, 1604, 6143, 1104, 2775, 2598, 4469, 5788, 6305, 1107, 1103, 1768, 1104, 9686, 6258, 1106, 8681, 1106, 6214, 2513, 1105, 11350, 119, 142, 20158, 16677, 1106, 2607, 117, 1114, 9582, 1757, 1755, 3776, 1105, 9147, 9816, 119, 142, 3190, 2101, 9637, 17444, 15517, 2036, 131, 118, 160, 9565, 2428, 142, 3190, 2101, 9637, 17444, 15517, 2036, 25777, 3998, 9666, 5513, 4659, 140, 21097, 118, 1351, 1381, 1106, 1357, 1381, 794, 16350, 1113, 2775, 9322, 7109, 111, 10331, 119, 794, 16350, 1113, 9059, 1718, 119, 142, 2137, 21986, 13821, 24805, 6143, 1104, 2775, 2598, 1107, 2775, 2598, 1107, 17308, 151, 119, 144, 119, 153, 22133, 11365, 16716, 9314, 17444, 15517, 2036, 18732, 23955, 17020, 2036, 118, 3291, 4060, 14602, 4474, 117, 5344, 10657, 17447, 17656, 15928, 141, 15779, 2101, 113, 13568, 1190, 122, 1214, 114, 117, 141, 12412, 113, 13568, 1190, 122, 1214, 114, 117, 142, 3190, 23258, 14962, 16523, 113, 13568, 1190, 122, 1214, 114, 117, 3670, 113, 13568, 1190, 122, 1214, 114, 117, 10722, 2249, 113, 13568, 1190, 122, 1214, 114, 5844, 17243, 21669, 11414, 12507, 15969, 2271, 9565, 8271, 21669, 11414, 17447, 17656, 15928, 131, 118, 794, 2508, 11655, 3375, 131, 10978, 3670, 117, 15945, 7230, 1105, 10978, 25059, 2486, 14501, 1106, 4795, 119, 794, 24222, 120, 156, 22705, 22073, 1116, 3707, 1105, 6156, 27430, 1158, 119, 794, 3998, 1158, 131, 157, 13113, 120, 14274, 117, 141, 12412, 117, 141, 15779, 2101, 117, 1105, 10722, 2249, 120, 22751, 2249, 119, 794, 17300, 10085, 1158, 11984, 3246, 1105, 9770, 3172, 5684, 120, 15274, 2812, 118, 12534, 119, 18630, 131, 120, 120, 7001, 119, 5750, 119, 3254, 120, 187, 120, 19892, 10732, 5345, 118, 1573, 7941, 22902, 4515, 120, 124, 1830, 1181, 1580, 1162, 1571, 2007, 1571, 23435, 19515, 1495, 1665, 1604, 136, 1110, 2386, 134, 1231, 1775, 118, 9133, 111, 178, 1377, 2246, 134, 9133, 118, 1499, 111, 1884, 134, 15969, 18630, 131, 120, 120, 7001, 119, 5750, 119, 3254, 120, 187, 120, 19892, 10732, 5345, 118, 1573, 7941, 22902, 4515, 120, 124, 1830, 1181, 1580, 1162, 1571, 2007, 1571, 23435, 19515, 1495, 1665, 1604, 136, 1110, 2386, 134, 1231, 1775, 118, 9133, 111, 178, 1377, 2246, 134, 9133, 118, 1499, 111, 1884, 134, 15969, 102]]

# print(tokenized_inputs0.word_ids(batch_index=0))
# [None, 0, 0, 0, 1, 1, 1, 1, 2, 3, 4, 5, 6, 7, 7, 7, 8, 9, 10, 11, 12, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 23, 23, 24, 25, 25, 25, 25, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 48, 49, 50, 51, 52, 53, 54, 54, 55, 56, 57, 58, 59, 60, 61, 61, 61, 61, 61, 61, 61, 62, 63, 64, 64, 64, 65, 65, 65, 65, 65, 65, 65, 66, 67, 68, 69, 70, 71, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 92, 92, 92, 92, 93, 94, 95, 96, 97, 98, 99, 100, 100, 101, 102, 103, 104, 105, 106, 106, 107, 108, 108, 108, 108, 109, 109, 109, 109, 110, 111, 111, 111, 111, 112, 113, 114, 115, 115, 115, 116, 116, 116, 117, 118, 119, 120, 121, 122, 123, 124, 124, 125, 126, 127, 128, 129, 130, 131, 132, 132, 132, 132, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 148, 149, 150, 151, 152, 153, 154, 155, 155, 155, 155, 155, 156, 156, 156, 156, 156, 156, 157, 157, 157, 158, 159, 160, 161, 161, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 179, 180, 180, 181, 182, 183, 184, 184, 185, 186, 187, 187, 188, 189, 189, 190, 191, 192, 193, 193, 194, 195, 195, 195, 196, 197, 198, 198, 199, 200, 200, 201, 202, 203, 204, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 229, 229, 230, 231, 231, 231, 231, 232, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 234, 235, 235, 236, 237, 237, 238, 239, 240, 241, 241, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 262, 262, 263, 264, 264, 264, 264, 265, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 267, 268, 268, 269, 270, 270, 271, 272, 273, 274, 274, 274, 275, 276, 277, 278, 279, 280, 281, 282, None]


""" ---------------- end of example ----------------"""

# !!!

tokenizer = DistilBertTokenizerFast.from_pretrained(
    'pre-trained-transformer-distilbert-base-cased/')


label_all_tokens = True


def tokenize_and_align_labels(tokenizer, examples, tags):
    """
    Arguments
    tokenizer: tokenizer from HuggingFace
    examples: [pre-processed tokenised text] from dataset (with '\n' replaced by ' '), len = batch size
    tags: list of tag ids, len = batch size
    """

    """# 1. tokenise raw input with huggingface tokeniser"""
    tokenized_inputs = tokenizer(
        examples, truncation=True, is_split_into_words=True, padding='max_length', max_length=512)
    # truncation=True: cuts sequences that exceed the maximum size allowed by our model
    # is_split_into_words: Whether or not the input is already pre-tokenized (e.g., split into words). If set to True, the tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace) which it will tokenize. This is useful for NER or token classification.
    # tokenized_inputs is a dictionary with 'input_ids' a list of token ids (of the huggingface tokeniser) for the tokenised words in the input 'examples', and 'attention_mask' a padding mask (0 for padded words and 1 otherwise)

    """# 2. assign the same the tag id for all sub-words in a word, store them in labels"""
    labels = []
    # recall tags is the list of tag id for each word of input 'examples' constructed from our labeled dataset, with whitespaces stripped
    # loop over the instances
    for i, label in enumerate(tags):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        # word_ids: a list that assigns, in ascending order, the same word id for all sub-words of the same original word aplit by the tokenizer

        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically ignored in the loss function.
            if word_idx is None:
                label_ids.append(-100)
            # get the label (tag id) for the first token (sub-word) of each word.
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            # For the other sub-words in the same word, we set the label to either the current label or -100, depending on the label_all_tokens flag.
            else:
                label_ids.append(label[word_idx] if label_all_tokens else -100)
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs


"""
# so now we have two columns: 'input_ids' that are the token ids for the sub-words of words split by huggingface tokenizer, 'labels' that are the corresponding tag ids belonging to the same words from the labeled dataset
# these two columns are used to further train our model
"""

tokenized_inputs_train = tokenize_and_align_labels(
    tokenizer, token_split_no_space_train, tags_train)
train_dataset = tf.data.Dataset.from_tensor_slices((
    tokenized_inputs_train['input_ids'],
    tokenized_inputs_train['labels']
))
X_train, y_train = tokenized_inputs_train['input_ids'], tokenized_inputs_train['labels']

tokenized_inputs_valid = tokenize_and_align_labels(
    tokenizer, token_split_no_space_valid, tags_valid)
valid_dataset = tf.data.Dataset.from_tensor_slices((
    tokenized_inputs_valid['input_ids'],
    tokenized_inputs_valid['labels']
))
X_valid, y_valid = tokenized_inputs_valid['input_ids'], tokenized_inputs_valid['labels']


# %% optimisation

"""
feed data into a pretrained ðŸ¤— model. optimize a DistilBERT model, which matches the tokenizer used to preprocess your data
"""


model = TFDistilBertForTokenClassification.from_pretrained(
    'pre-trained-transformer-distilbert-base-cased/', num_labels=len(unique_tags))


# import tensorflow_addons as tfa
# tfa.metrics.F1Score(num_classes=len(unique_tags), threshold = 0.5)

callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy', mode='max', patience=30, min_delta=0.00001, restore_best_weights=True)

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-6)
model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=[
              'accuracy'])  # can also use any keras loss fn
# if you get GPU 'Resource exhausted: failed to allocate memory', reduce batch size
history = model.fit(X_train, y_train,
                    validation_data=(X_valid, y_valid),
                    epochs=200, callbacks=[callback], batch_size=4)

# Epoch 108/200
# 39/39 [==============================] - 7s 187ms/step - loss: 0.0066 - accuracy: 0.8575 - val_loss: 0.3992 - val_accuracy: 0.7552
# !!! caution: imbalanced class 'Empty'


# model.save_weights('transformer_model_resume.h5')

model.evaluate(X_train, y_train)
# [0.015459313057363033, 0.8556336164474487]
model.evaluate(X_valid, y_valid)
# [0.3139658272266388, 0.7595880627632141]


history_df = pd.DataFrame(history.history)
history_df.columns


fig = plt.figure(dpi=150)
plt.plot(history.history['loss'], color='red', label='loss')
plt.plot(history.history['val_loss'], color='blue', label='val_loss')
ax = plt.gca()
ax.set_xlabel('Epochs')
ax.set_ylabel(None)
# ax.set_yticks([])
ax.set_title(None)
ax.spines['top'].set_visible(False)
# ax.spines['left'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.legend()
# plt.savefig('transformer_loss', dpi = 150)

fig = plt.figure(dpi=150)
plt.plot(history.history['accuracy'], color='red', label='accuracy')
plt.plot(history.history['val_accuracy'], color='blue', label='val_accuracy')
ax = plt.gca()
ax.set_xlabel('Epochs')
ax.set_ylabel(None)
# ax.set_yticks([])
ax.set_title(None)
ax.spines['top'].set_visible(False)
# ax.spines['left'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.legend()
# plt.savefig('transformer_accuracy', dpi = 150)


# %% evaluate F1 score for the labeled datasets

# specify labeled dataset here
tags_labeled = tags_valid
tokenized_inputs_labeled = tokenized_inputs_valid.copy()


# just run the following
labels_labeled = [[id2tag.get(l) for l in tag] for tag in tags_labeled]
batch_size_labeled = len(tokenized_inputs_labeled['input_ids'])
X_labeled, y_labeled = tokenized_inputs_labeled['input_ids'], tokenized_inputs_labeled['labels']


word_ids_labeled = np.array([tokenized_inputs_labeled.word_ids(
    i) for i in range(len(tokenized_inputs_labeled['input_ids']))])

word_ids_labeled[:, 0] = -1
# array([[-1, 0, 0, ..., None, None, None],
#        [-1, 0, 0, ..., None, None, None]], dtype=object)
# use 4096 because 4096 > 512 = MAX_LEN
word_ids_labeled = np.where(word_ids_labeled == None, 4096, word_ids_labeled)

# from word_ids_labeled, we identify the location of the start of each word
word_ids_unique_index_labeled = []
for i in range(batch_size_labeled):
    start_of_word_locations = np.unique(
        word_ids_labeled[i], return_index=True)[1]
    # if the last word is the padding token, drop it
    if word_ids_labeled[i][start_of_word_locations[-1]] == 4096:
        start_of_word_locations = start_of_word_locations[:-1]
    word_ids_unique_index_labeled.append(start_of_word_locations)


# predict
prediction_labeled = model.predict(X_labeled)
# apply softmax to the precited logits and take the most probable label
pred_tags_labeled = []
soft_labeled = tf.nn.softmax(prediction_labeled.logits)
pred_tag_ids_labeled = tf.math.argmax(soft_labeled, axis=-1).numpy()
for i in range(batch_size_labeled):
    pred_tags_labeled.append([id2tag[pred_tag_ids_labeled[i, j]]
                             for j in range(pred_tag_ids_labeled.shape[1])])
pred_tags_labeled = np.array(pred_tags_labeled)

# from the predicted sub-word labels, for each word take the label of the first sub-word, and map from tag ids to the tags
pred_tags_words_labeled = []
for i in range(batch_size_labeled):
    pred_tags_i_labeled = pred_tags_labeled[i]
    # remove the first tag corresponding to <start of sentence>
    word_ids_unique_index_i = word_ids_unique_index_labeled[i][1:]
    pred_tags_words_labeled.append(
        [pred_tags_i_labeled[ind] for ind in word_ids_unique_index_i])

pred_tags_words_labeled = pd.Series(
    pred_tags_words_labeled, name='predicted tags')
y_pred_labeled = pred_tags_words_labeled.to_list()


# some instances are truncated upon tokenisation by Bert
truncated_instances = [i for i, pred in enumerate(
    y_pred_labeled) if len(pred) != len(labels_labeled[i])]

# len(truncated_instances)
# Out[98]: 105
# len(y_pred_labeled[1])
# Out[93]: 330
# len(labels_labeled[1])
# Out[92]: 369
# len(tokenizer(token_split_no_space_train[1], truncation=False, is_split_into_words=True, padding='max_length', max_length=512)['input_ids'])
# 558

# compute the scores for the [truncated] instances, i.e. we truncate the given true labels
for i in truncated_instances:
    labels_labeled[i] = labels_labeled[i][:len(y_pred_labeled[i])]

# [i for i, pred in enumerate(y_pred_labeled) if len(pred) != len(labels_labeled[i])]
# []

X_valid, y_valid = tokenized_inputs_valid['input_ids'], tokenized_inputs_valid['labels']

tokenized_inputs_valid['word_ids']


# https://huggingface.co/metrics/seqeval
# https://pypi.org/project/seqeval/

# compute label-wise precision, recall and f1 scores, [after] re-combining the sub-word tokens
print(classification_report(labels_labeled, y_pred_labeled))

# training set
#                     precision    recall  f1-score   support
#
#                Name      1.00      1.00      1.00       155
# Years of Experience      1.00      1.00      1.00        33
#              Degree      0.76      0.81      0.78       105
#         Designation      0.97      0.96      0.97       284
#              Skills      0.97      0.99      0.98      2128
#       Email Address      0.99      1.00      0.99      1804
#               Empty      1.00      1.00      1.00     42167
#            Location      0.99      0.99      0.99       239
#        College Name      0.77      0.87      0.82       101
# Companies worked at      0.97      0.96      0.97       336
#     Graduation Year      0.93      0.67      0.78        82

#          micro avg       0.99      1.00      1.00     47434
#          macro avg       0.94      0.93      0.93     47434
#       weighted avg       0.99      1.00      1.00     47434


# validation set
#                Name      0.97      0.95      0.96        64
# Years of Experience      0.54      0.64      0.58        11
#              Degree      0.59      0.64      0.61        53
#         Designation      0.61      0.70      0.65       123
#              Skills      0.93      0.73      0.82      1426
#       Email Address      0.86      0.89      0.88       682
#               Empty      0.96      0.98      0.97     16042
#            Location      0.81      0.72      0.76        90
#        College Name      0.41      0.61      0.49        56
# Companies worked at      0.67      0.63      0.65       139
#     Graduation Year      0.67      0.36      0.47        33

#          micro avg       0.94      0.95      0.94     18719
#          macro avg       0.73      0.71      0.71     18719
#       weighted avg       0.94      0.95      0.94     18719


# %% prediction of unlabeled datasets

"""
# the model takes as input a list containing a list of the tokenized 'input_ids' for each instance and gives the predicted logits (i.e. without softmax)
"""

# take the first 3 lines of the training json dataset and try to predict the labels

token_split_no_space_test = tokentize_dataset(
    convert_dataturks_to_spacy_unlabeled("resume/lineone.json"))
batch_size = len(token_split_no_space_test)

# tokenizer.tokenize(df_data0['content'].values.tolist())
tokenized_inputs_test = tokenizer(token_split_no_space_test, truncation=True,
                                  is_split_into_words=True, padding='max_length', max_length=512)

tokenized_inputs_test_input_ids = tokenized_inputs_test['input_ids']

# construct the word_ids np.array, replacing the first (start of sentence) by -1, and None by 4096
word_ids = []
for i in range(batch_size):
    word_ids.append(tokenized_inputs_test.word_ids(batch_index=i))
word_ids = np.array(word_ids)
word_ids[:, 0] = -1
# array([[-1, 0, 0, ..., None, None, None],
#        [-1, 0, 0, ..., None, None, None]], dtype=object)
# use 4096 because 4096 > 512 = MAX_LEN
word_ids = np.where(word_ids == None, 4096, word_ids)

# from word_ids, we identify the location of the start of each word
word_ids_unique_index = []
for i in range(batch_size):
    word_ids_unique_index.append(np.unique(word_ids[i], return_index=True)[1])

# returns [logits] for each sub-word
prediction = model.predict(tokenized_inputs_test_input_ids)

# apply softmax to the precited logits and take the most probable label
pred_tags = []
soft = tf.nn.softmax(prediction.logits)
pred_tag_ids = tf.math.argmax(soft, axis=-1).numpy()
for i in range(batch_size):
    pred_tags.append([id2tag[pred_tag_ids[i, j]]
                     for j in range(pred_tag_ids.shape[1])])
pred_tags = np.array(pred_tags)
# array([['Empty', 'Name', 'Name', ..., 'Empty', 'Empty', 'Empty'],
#        ['Name', 'Name', 'Name', ..., 'Empty', 'Empty', 'Empty'],
#        ['Name', 'Name', 'Name', ..., 'Empty', 'Empty', 'Name']],
#       dtype='<U19')


# from the predicted sub-word labels, for each word take the label of the first sub-word, and map from tag ids to the tags
pred_tags_words = []
for i in range(batch_size):
    pred_tags_i = pred_tags[i]
    # remove the first tag corresponding to <start of sentence>
    word_ids_unique_index_i = word_ids_unique_index[i][1:]
    pred_tags_words.append([pred_tags_i[ind]
                           for ind in word_ids_unique_index_i])

pred_tags_words = pd.Series(pred_tags_words, name='predicted tags')

# 0    [Name, Name, Designation, Designation, Designation, Empty, Companies worked at, Location, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Designation, Designation, Designation, Companies worked at, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, ...]
# 1                                                                    [Name, Name, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Location, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, ...]
# 2                                                     [Name, Name, Name, Location, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, ...]
# Name: predicted tags, dtype: object


# compare to the true labels


token_split_no_space_test, labels_test = tokentize_dataset_tags(
    convert_dataturks_to_spacy("resume/lineone.json"))

pd.Series(labels_test)
# 0    [Name, Name, Designation, Designation, Designation, Empty, Companies worked at, Location, Empty, Empty, Empty, Empty, Empty, Empty, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Designation, Designation, Designation, Companies worked at, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, ...]
# 1                                                                                    [Name, Name, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Location, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, ...]
# 2                                                                     [Name, Name, Name, Location, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Email Address, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, Empty, ...]
# dtype: object
